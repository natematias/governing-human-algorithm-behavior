# COMM 4940: Investigating Alleged AI Failures
[J. Nathan Matias](https://natematias.com) &lt;nathan.matias AT cornell.edu&gt; , Department of Communication, Field Member, Information Science

<a href="https://en.wikipedia.org/wiki/Humanetics"><img alt="he Q10 is an anthropomorphic test device (ATD) that represents a ten year old child for crash test simulation." style="width:30%; float:right; margin-left:10px; margin-bottom:10px; border:3px solid #ccc;" src="Child_ATD_-_Q10.jpg"/></a>

* Spring 2026 (see course listing)
* Time: DAY & DAY TIME - TIME  
* Location: TBD


<!--![Paradise Wildfires, YouTube algorithms, Wilmer Catalan-Ramirez](https://imgur.com/R5pWrV1.png) -->
<!--<small style="opacity:0.3"><em>Images: <a href="https://www.prnewswire.com/news-releases/googles-waze-app-blocks-wildfire-escape-route-for-residents-living-in-the-los-gatos-hills-and-historic-neighborhoods-300913095.html">Google Waze blocks wildfire escape route</a>, <a href="https://www.media.mit.edu/publications/the-international-affiliation-network-of-youtube-trends/">The international affiliation network of YouTube Trends</a>, and <a href="https://www.chicagotribune.com/news/ct-met-immigrant-released-jail-0126-story.html">Immigrant released from detention after being wrongly listed as gang member</a>.</em></small>
-->
AI systems that monitor and interact with people are everywhere—directing the behavior of law enforcement, giving us relationship advice, managing the world's financial systems, shaping our cultures, and flipping a coin on the success or failure of movements for change. How can we know if they are any good, and if accusations of their harms are true?

In 2023, this challenge became even clearer when Attorney General Letitia James and 31 other state Attorneys General filed a federal lawsuit against Meta for allegedly "[harming young people's mental health and contributing to the youth mental health crisis](https://ag.ny.gov/press-release/2023/attorney-general-james-and-multistate-coalition-sue-meta-harming-youth)." One of the lawsuit's claims is that Meta, through Facebook and Instagram, encouraged compulsive use of the platform through its algorthms and design features—contributing to a widespread mental health crisis. In the next two years as generative AI was deployed to millions of people, people started to raise the alarm that chatbots were also [contributing to deaths by suicide](https://www.nytimes.com/2025/10/24/magazine/character-ai-chatbot-lawsuit-teen-suicide-free-speech.html). In response, company CEOs sometimes argued that deeper social problems, not their products were to blame.


In this 3 credit course for (15) upper-level undergraduates and up to (5) PhD students, you will learn about the design of interactive algorithms and the feedback patterns they create with human behavior. You will learn about the challenge they represent for social policy, ways to research their behavior, debates over how to identify AI failures, and emerging policy ideas for investigating and governing these complex patterns. Along the way, you will hear from pioneers in policy, advocacy, and scholarship.

This course is an excellent stepping stone for anyone interested in a career in policy, advocacy, academia, or industry research.

This course requires students to have taken introductory statistics and be able to conduct multiple regression. This could include ILRST2110, AEM 2100, CRP 1200, ENGRD 2700, HADM 2010, MATH 1710, PUBPOL 2100, PUBPOL 2101, PSYCH 2500, SOC 3010, STSCI 2100, STSCI 2150, BTRY 3020, STSCI 2110, STSCI 3200. If you have any questions about this requirement, please contact the instructor.

<div style="background:#eee; border-radius:15px; border:2px solid #ddd; margin:15px; padding:10px">

### How to Register for this class

#### Upper Level Undergraduates
To join this class, register for COMM 4940 like any class.</p>

#### PhD Students
To join this class, you have two options:</p>

<ol><li>1. Enroll in an <strong>independent study</strong> (COMM 7970) with Professor Matias and then attend the class (preferred). Before enrolling, please write a brief note to Professor Matias with "Enrolling in COMM [4940]" in the subject. The email should:<ul>
<li>Introduce yourself, your program, and where you are at (1-2 sentences)</li>
<li>Explain why you want to take the course. For example, is it related to your research? An interest in policy? A project you want to develop in the course? (2-3 sentences)</li>
<li>Describe the perspectives and skills you bring to the class (a few bullet points) (<em>for example, an understanding of certain theory, or capabilities at qualitative research, data analysis, policy writing, advocacy, media-making, or software development</em>)</li></ul></li>
<li>2. Enroll in COMM 4940 as a piece of <strong>elective coursework</strong></li>
</ol>
</div>

## What You Learn
In this seminar class, you will engage with the science and policy challenges of regulating human-algorithm behavior. Along the way, you will work with scientific issues of methods, theory, and ethics, policy questions about how to govern these situations, and how to bridge between science and policy in a democracy. For a final project, student teams (3-4) will produce a novel project that makes a contribution at the intersection of science and policy.

By the end of the semester, students will be able to:

- Identify, analyze, and evaluate claims about how human and AI behavior interact
- Summarize and criticize major approaches to making claims about AI failures, from the perspective of computer science and the social sciences
- Describe and evaluate major governance approaches
- Design and analyze research methodologies for for transparency, accountability, and product improvement
- Understand the uses of social science and computer science research in the policy process
- Design an intervention into scholarly and policy conversations on alleged AI failures

PhD students are encouraged to connect the course to an existing research question that they wish to connect with policy, or to develop a project that could become part of their wider research. In addition to the above learning outcomes, students will be able to:

- Identify ways to link their research interests to policy
- Design studies that can inform policy processes beyond their scholarly field

## Activities

*Weekly Activities*:  Throughout the semester, students will read a selection of articles and discuss that reading in class and on Canvas. Once teams have been formed, students will also submit regular progress reports on their final project.

*Algorithm Incident Report*: The midterm is an analysis of an algorithm-involved event in the news, based on publicly-available information.

*Project Proposal*: Your project proposal will include a description of the project, a bibliography, a list of the roles that team members will play, and a timeline.

*Final Project*: An intervention into academic and/or policy conversations on human-algorithm research. In 2024, the class will support projects in two areas:

* An in-depth case study and/or forensic analysis of some alleged AI system failure ([see the AI Incident Database for ideas](https://incidentdatabase.ai/about/))
* A proposal for a "black box" or data schema of what to record to inform future analysis of incidents
* A research project of your own choosing, if your team includes a PhD student

*Grading*: Participation in class & online: 30%. Midterm: 30%. Final project: 40%.

## About the Instructor
<img src="https://natematias.com/images/profile.png" style="margin-right:10px;" align="left" width="150">

<a href="https://natematias.com">Dr. J. Nathan Matias</a>
(<a href="http://social.coop/@natematias">@natematias</a>) 
<meta http-equiv="content-type" content="text/html; charset=UTF-8"> organizes community behavioral science for a safer, fairer, more understanding internet. Nathan is an assistant professor in the Cornell University Department of Communication. He is also a field member in Information Science.

Nathan is the founder of the <a href="citizensandtech.org/">Citizens and Technology Lab</a>, a public-interest project at Cornell that supports community-led behavioral science—conducting independent, public-interest audits and evaluations of social technologies. CAT Lab achieves this through software systems that coordinate communities to conduct their own research on social issues. Nathan has worked with communities of millions of people to test ideas to <a href="https://www.pnas.org/doi/10.1073/pnas.1813486116" style="">prevent online harassment</a>, <a href="vimeo.com/natematias/followbias">broaden gender diversity</a> on social media, <a href="https://www.nature.com/articles/s41598-023-38277-5">manage human/algorithmic misinformation</a>, and <a href="https://citizensandtech.org/2021/11/crowdsourced-audit-studies/">audit algorithms</a>.</p>

### Office Hours
I am available for office hours in person and online during the following times:

* DAY (TIME-TIME)
* DAY (TIME-TIME)

Please feel free to show up or [schedule in advance](https://calendarbridge.com/book/natematias/office-hours). The most productive office hours involve coming to my office with a sense of your question and all of the needed information ready to go. I also enjoy just having interesting conversations with students about things you're interested in as well!

## Schedule

<!-- 19 sessions then spring break-->
<!-- then 8 sessions to the end of classes-->

#### 1.1 What is an AI Failure and Why Does it Matter? (Intro class)
This class session will introduce the course and pose initial questions. 

* Sophie Mellor (2022) [After a 14-year-old Instagram user's suicide, Meta apologizes for (some of) the self-harm and suicide content she saw](https://finance.yahoo.com/news/14-old-instagram-users-suicide-151102166.html). Yahoo! News.

---

#### 1.2 Tech and Methods: How Do AI Systems Work? What Does It Mean to Fail?
In this class, we will discuss the code and mathematics behind adaptive algorithms, as well as what it means for an AI system to fail.

* Paresh Dave (2023) [The 5 Instagram Features That US States Say Ruin Teens’ Mental Health](https://www.wired.com/story/the-5-instagram-features-that-us-states-say-ruin-teens-mental-health/). WIRED
* Narayanan, A. (2023). [Understanding social media recommendation algorithms](https://knightcolumbia.org/content/understanding-social-media-recommendation-algorithms) . Knight First Amendement Institute.

Graduate student reading:

* Angel, M. P., & boyd, d. (2024, March). [Techno-legal solutionism: Regulating children's online safety in the United States](https://dl.acm.org/doi/pdf/10.1145/3614407.3643705). In Proceedings of the 2024 Symposium on Computer Science and Law (pp. 86-97).

<!-- * Shelby, R., Rismani, S., Henne, K., Moon, A., Rostamzadeh, N., Nicholas, P., ... & Virk, G. (2023, August). [Sociotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction](https://dl.acm.org/doi/10.1145/3600211.3604673). In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society (pp. 723-741).
-->
<!--* Georgia Wells, Jeff Horwitz, and Deepa Seetharaman (2021) [Facebook Knows Instagram Is Toxic for Teen Girls, Company Documents Show](https://www.wsj.com/articles/facebook-knows-instagram-is-toxic-for-teen-girls-company-documents-show-11631620739). Wall Street Journal.-->

<!--Graduate student reading: 

* Angel, M. P., & boyd, d. (2024, March). [Techno-legal solutionism: Regulating children's online safety in the United States](https://dl.acm.org/doi/pdf/10.1145/3614407.3643705). In Proceedings of the 2024 Symposium on Computer Science and Law (pp. 86-97).-->


<!--Shardanand, U., & Maes, P. (1995, May). [Social information filtering: Algorithms for automating “word of mouth”](https://dl.acm.org/doi/10.1145/223904.223931). In Proceedings of the SIGCHI conference on Human factors in computing systems (pp. 210-217).
-->
<!--* Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., & Riedl, J. (1994, October). [Grouplens: An open architecture for collaborative filtering of netnews](https://dl.acm.org/doi/abs/10.1145/192844.192905). In Proceedings of the 1994 ACM conference on Computer supported cooperative work (pp. 175-186).-->

Questions to consider:

* What are the specific claims being made about Meta and mental health?
* How does the description of the systems underlying the "algorithm" change how you think about how harms might occur?
* What harms are so important that they deserve investigation and redress by designers, courts, or law? What harms are not?
* How do we make sense of claims that the attempt to attribute harm to technology is an unhelpful form of techno-determinism?

---

#### 2.1 Feedback Loops: Simple Models to Explain Complex Behaviors
* Clegg, N. (2021, March 31). [You and the Algorithm: It Takes Two to Tango](https://nickclegg.medium.com/you-and-the-algorithm-it-takes-two-to-tango-7722b19aa1c2). Medium.
* Kimmerer, R. (2013) "Windigo Footprints" from Braiding Sweetgrass. Milkweed Editions

Graduate student reading:

* Matias, J.N. (2023) [Humans and algorithms work together — so study them together](https://www.nature.com/articles/d41586-023-01521-z). Nature.

Questions to think about from Windigo Footprints:

* Think about an event related to algorithms and AI systems that matters to you. It might be an incident related to mental health, or discrimination, or something else
* Imagine the forces akin to "hunger" (the drive for more) and "gratitude" (a drive for less of something) that direct individuals and institutions toward **more** and which ones direct them toward **less** 
* Reflect on how those forces might have contributed to that event.
* How persuasive do you find Clegg's argument that companies should be held less responsible for harms since their systems react to human behavior?

---

#### 2.2 Incidents and Disasters
In this class, we're going to ask the question "What is an incident" and "What is an accident/disaster/catastrophe?" while also discussing how understanding them might help society:
Readings: 

* Marsh, Allison (2021) [The Inventor of the Black Box Was Told to Drop the Idea and “Get On With Blowing Up Fuel Tanks”](https://spectrum.ieee.org/the-inventor-of-the-black-box-was-told-to-drop-the-idea-and-get-on-with-blowing-up-fuel-tanks). IEEE Spectrum.
* Pinch, T. J. (1991). [How do we treat technical uncertainty in systems failure? The case of the space shuttle Challenger. In Social responses to large technical systems: control or anticipation](https://link.springer.com/content/pdf/10.1007/978-94-011-3400-2.pdf) (pp. 143-158). Dordrecht: Springer Netherlands.

Graduate student reading:

* Knowles, S. (2014). [Engineering risk and disaster: Disaster-STS and the American history of technology](https://researchdiscovery.drexel.edu/esploro/outputs/journalArticle/Engineering-Risk-and-Disaster-Disaster-STS-and/991019167743004721). Engineering Studies, 6(3), 227-248.

Questions to think about — choose a specific risk, harm, or incident involving AI, and consider:

- Based on the article by Pinch, list out at least six of the "causes" of the Challenger disaster, and label them (were they technical, political social, etc). Could you name the most important one?
- List out things that might possibly go wrong
- What might you want to record in order to go back and figure out what went wrong?

<!--#### Thurs 02-01 Case Study: Teenage Mental Health
To build our intuitions about governing algorithms and what to do about it, we're going to consider a series of classes about discrimination online. This class introduces the first case study, related to allegations that Facebook and Instagram contributed to a teen mental health crisis.

Readings: TBD-->

<!--#### Thurs 02-24 Topic area: Algorithms, Self-Harm, and Eating Disorders
In this topic area session, we will discuss the role of algorithms in eating disorders and self-harm.

* Seetharaman, G. W., Jeff Horwitz and Deepa. (2021, September 14). [Facebook Knows Instagram Is Toxic for Teen Girls, Company Documents Show](https://www.wsj.com/articles/facebook-knows-instagram-is-toxic-for-teen-girls-company-documents-show-11631620739). Wall Street Journal.
* Carvajal, D. (2008, April 15). [French legislators approve law against Web sites encouraging anorexia and bulimia](https://www.nytimes.com/2008/04/15/world/europe/15iht-paris.4.12015888.html). The New York Times. 
* (gradstudents) Chancellor, S., Pater, J. A., Clear, T., Gilbert, E., & De Choudhury, M. (2016, February). [#thyghgapp: Instagram content moderation and lexical variation in pro-eating disorder communities](https://dl.acm.org/doi/abs/10.1145/2818048.2819963). In Proceedings of the 19th ACM conference on computer-supported cooperative work & social computing (pp. 1201-1213).-->


<!-----


#### Tues 02-06 Case Study: Algorithms and Discrimination
To build our intuitions about governing algorithms and what to do about it, we're going to consider a series of classes about discrimination online. This class introduces the problem of discrimination in online labor markets.

* Mullainathan, S. (2019). [Biased algorithms are easier to fix than biased people](https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html). The New York Times.
* McGhee, H. (2021). "Racism Drained the Pool" from The sum of us: What racism costs everyone and how we can prosper together. One World.
 * Read pages 17 - 28 (Cornell Library has e-book copies)
* Carville, O. (2019) Meet Murray Cox, [The Man Trying to Take Down Airbnb](https://www.bnnbloomberg.ca/meet-murray-cox-the-man-trying-to-take-down-airbnb-1.1263088). Bloomberg
<!--* (gradstudents) Edelman, B. G., & Luca, M. (2014). [Digital discrimination: The case of Airbnb. com.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2377353) Harvard Business School NOM Unit Working Paper, (14-054).-->

<!--Questions to think about for class and discussion:

* How does Mullainathan think about discrimination and bias?
* In Mullainathan's view, what does it mean to "fix" bias?
* How does McGhee think about discrimination and bias?
* How might McGhee think about "fixing" discrimination and bias?
* What do we gain and lose by redefining our understanding of social issues only to the things that algorithms can meaningfully address?
 -->

---

#### 3.1 The Lifecycle of a Population Harm in Democracy
In this class, we will consider the lifecycle of a conversation about harms over the long arc. Choose one of the following stories to learn more about, and link up with other students looking into the same case study.

* **Air pollution**, environmental toxicology, and the clean air act:
	* Jacobs, E. T., Burgess, J. L., & Abbott, M. B. (2018). [The Donora smog revisited: 70 years after the event that inspired the clean air act](https://ajph.aphapublications.org/doi/full/10.2105/AJPH.2017.304219). American journal of public health, 108(S2), S85-S88.
	* Costa, D., & Gordon, T. (2000). [Mary O. Amdur](https://academic.oup.com/toxsci/article-abstract/56/1/5/1646040). Toxicological Sciences, 56(1), 5-7.
* **Automotive safety** and crash injury studies:
	* Gangloff, A. (2013). [Safety in accidents: hugh dehaven and the development of crash injury studies](https://muse.jhu.edu/pub/1/article/500796/summary). Technology and Culture, 54(1), 40-61.
	* Mars, R. (2017) "[The Nut Behind the Wheel](https://99percentinvisible.org/episode/nut-behind-wheel/)." 99% Invisible
* **Nuclear testing**, radiation exposure, and Lukemia
	* Documentary:[Downwinders and the Radioactive West](https://www.youtube.com/watch?v=77gJtV13ouo). PBS Utah
	* Machado, S. G., Land, C. E., & McKay, F. W. (1987). Cancer mortality and radioactive fallout in southwestern Utah. American journal of epidemiology, 125(1), 44-61.
	* Beck, H. L., & Krey, P. W. (1983). Radiation exposures in Utah from Nevada nuclear tests. Science, 220(4592), 18-24.
* **Smoking** and lung cancer:
	* Parascandola, M. (2004). [Two approaches to etiology: the debate over smoking and lung cancer in the 1950s](https://www.sciencedirect.com/science/article/pii/S0160932704000444). Endeavour
	* Documentary: (2015) [Merchants of Doubt](https://catalog.library.cornell.edu/catalog/16815224). Sony Pictures Classics.
* **Per- and polyfluoroalkyl substances (PFAS)**:
	* Lerner, S. (2024) [Toxic Gaslighting: How 3M Executives Convinced a Scientist the Forever Chemicals She Found in Human Blood Were Safe](https://www.propublica.org/article/3m-forever-chemicals-pfas-pfos-inside-story). ProPublica.
	* Film: (2019) [Dark Waters](https://catalog.library.cornell.edu/catalog/15713089). Focus Features.

For whichever of these stories you choose, the group of students studying the same case should create a bullet point timeline of:

  * When did people first start worrying about the problem?
  * When did people start collecting data about the problem?
  * Who stood to gain or lose from the results of the decision?
  * What institutions faced important decisions (in companies, governments, courts, etc)
  * How where scientists brought into the issue?
  * How did scientists handle disagreements?
  * If any consensus was reached on the issue, how long did it take?
  * How long did it take between the first expression of concern and a systemic solution to the problem?

---


#### 3.2 Missatribution and Mis-estimation of Harm
While unaddressed harm can lead to compounding problems over time, misattribution and mis-estimation of harms can also create serious problems. It can lead to injustice toward accused product creators and distract attention from the real problem. The consequences of a false positive for institutions and people's lives can cause irreversible harm and can last for generations. In this class, we will discuss what happens when scientists and policymakers are wrong about the causes of an evident harm.

* Pierson, E., Li, S. (2015) [A better way to gauge how common sexual assault is on college campuses](https://www.washingtonpost.com/news/grade-point/wp/2015/10/15/a-better-way-to-gauge-how-common-sexual-assault-is-on-college-campuses/). The Washington Post
* Gantman, A. P., & Paluck, E. L. (2025). [What is the psychological appeal of the serial rapist model? Worldviews predicting endorsement](https://www.cambridge.org/core/journals/behavioural-public-policy/article/what-is-the-psychological-appeal-of-the-serial-rapist-model-worldviews-predicting-endorsement/732E0DC20E4742A09FEB013E5D466C96). Behavioural Public Policy, 9(2), 461-476.

Graduate student reading (choose one):

* Rosenberg, M., Townes, A., Taylor, S., Luetke, M., & Herbenick, D. (2019). [Quantifying the magnitude and potential influence of missing data in campus sexual assault surveys: A systematic review of surveys, 2010–2016](https://www.tandfonline.com/doi/full/10.1080/07448481.2018.1462817). Journal of American college health, 67(1), 42-50.
* Porat, R., Gantman, A., Green, S. A., Pezzuto, J. H., & Paluck, E. L. (2024). [Preventing sexual violence: A behavioral problem without a behaviorally informed solution](https://journals.sagepub.com/doi/full/10.1177/15291006231221978). Psychological Science in the Public Interest, 25(1), 4-29.

---


#### 4.1 Making Empirical Claims About Technology Harms 
In this two-part series, we will discuss a case study in an alleged algorithm failure. Students will choose among one of the following datasets for project teams to analyze. Teams will be assigned to one of two sides: the side defending technology makers, and the side making an accusation about a technology failure.

Before the class, students will read a paper related to the topic and come prepared to discuss the broader issue and receive information on how to analyze the dataset.


* Primary plan: Data from the European Union on the number of child safety reports for different online platforms

* Backup (if above data collection proves impossible):
   * Abdel-Aty, M., & Ding, S. (2024). [A matched case-control analysis of autonomous vs human-driven vehicle accidents](https://www.nature.com/articles/s41467-024-48526-4#data-availability). Nature communications, 15(1), 4931.
   
---

#### 4.2 Analyzing Archives of AI Failures

In this class, teams will present competing analyses to try to convince the class of their point of view.

We will then discuss what we learned from the experience.

<!--* Eckhouse, L., Lum, K., Conti-Cook, C., & Ciccolini, J. (2019). [Layers of bias: A unified approach for understanding problems with risk assessment](https://journals.sagepub.com/doi/abs/10.1177/0093854818811379). Criminal Justice and Behavior, 46(2), 185-209.
-->

<!--
* Matias, J.N. (2023) [Humans and algorithms work together — so study them together](https://www.nature.com/articles/d41586-023-01521-z). Nature.
* Orben, A., & Matias, J. N. (2025). [Fixing the science of digital technology harms](https://www.science.org/doi/full/10.1126/science.adt6807). Science, 388(6743), 152-155.-->

<!--* Mengersen, K., Moynihan, S. A., & Tweedie, R. L. (2007). [Causality and association: The statistical and legal approaches](https://www.jstor.org/stable/27645822). Statistical Science, 227-254.

---

#### Thurs 02-15 Data, Evidence, and Strategic Litigation
In this class, we will hear from someone working on litigation alleging that Meta should be held responsible for a mental health impact on teenagers.

* Reading: TBD

#### Thurs 02-10 Case Study: Bias, Fairness, and Human-Algorithm Feedback
Common discussions of algorithm governance focus on decision-making, which is a simpler problem than human-algorithm feedback. In this class, we will discuss this difference between the two.

* Ekstrand, M. D., Das, A., Burke, R., & Diaz, F. (2021). "The Problem Space" from [Fairness and Discrimination in Information Access Systems](https://arxiv.org/abs/2105.05779). arXiv preprint arXiv:2105.05779.
* Matias, J.N. (2021) [Nudging Algorithms by Influencing Humans](https://osf.io/m98b6/) (under review).
* (gradstudents) Ekstrand, M. D., Das, A., Burke, R., & Diaz, F. (2021). "Dynamic Fairness" from [Fairness and Discrimination in Information Access Systems](https://arxiv.org/abs/2105.05779). arXiv preprint arXiv:2105.05779.
-->
<!--* (gradstudents) Licklider, J. C. (1960). [Man-computer symbiosis. IRE transactions on human factors in electronics](https://groups.csail.mit.edu/medg/people/psz/Licklider.html), (1), 4-11.
  *  For historical context: Mindell, David A. (2002) [Cybernetics](http://web.mit.edu/esd.83/www/notebook/Cybernetics.PDF). From Research Seminar in Engineering Systems http://web.mit.edu/esd.83/www/notebook/Cybernetics.PDF -->

---

#### 5.1 What is an AI Incident?
In order for algorithms to be governed, we need to be able to describe what happens when one goes wrong. So what is an incident anyway, and how would we establish that harm had occurred?

<!--* Allen, J. (2024) [Why Is Instagram Search More Harmful Than Google Search?](https://integrityinstitute.org/blog/why-is-instagram-search-more-harmful-than-google-search). Integrity Institute
-->
* Matias et al (under review). Accelerating the Science of Digital Harms through Data Donations to a Digital Harms Research Center. 
* McGregor, S. (2021, May). [Preventing repeated real world AI failures by cataloging incidents: The AI incident database](https://ojs.aaai.org/index.php/AAAI/article/view/17817). In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 17, pp. 15458-15463).

Graduate Student Reading:

* Singh, G., Patel, R. H., Vaqar, S., & Boster, J. (2024). [Root cause analysis and medical error prevention](https://www.ncbi.nlm.nih.gov/books/NBK570638/). In StatPearls. StatPearls Publishing.

To prepare for class, think about the story by Sophie Mellor that we read at the beginning of class. Imagine that you are an employee at Instagram who is asked to investigate a mental-health related incident experienced by a young person who was an Instagram user.

* create a definition for the incident type— what is it and what isn't it? How would you tell what counts as an incident or not?
* make a list of 6 things that you would want to record from what might be know by Instagram, friends/family, and institutions

In class, we will work to collaborate on an incident report template for several kinds of mental health related incidents.

---

#### 5.2 Discussing Final Project Ideas
In this class, we will discuss possible final project ideas for the course.

In preparation for the class, meet with a possible project team to develop ideas for collective feedback and discussion.

As a reminder, final project ideas include one of three options:

* An in-depth case study and/or forensic analysis of some failure of an adaptive algorithm ([see the AI Incident Database for ideas](https://incidentdatabase.ai/about/))
* A proposal for a "black box" or data schema of what to record to inform future analysis of incidents
* An independent project involving a graduate student team member

---
FEBRUARY BREAK
---

#### 6.2 Methods: Auditing and Red Teaming an AI System
How do researchers identify systemic failures in the behavior of an AI system? Audits and Red Teaming are two popular methods.

* Metaxa, D., Park, J. S., Robertson, R. E., Karahalios, K., Wilson, C., Hancock, J., & Sandvig, C. (2021). "Introduction to Auditing" [Auditing algorithms: Understanding algorithmic systems from the outside in](https://www.nowpublishers.com/article/Details/HCI-083). Foundations and Trends® in Human–Computer Interaction, 14(4), 272-344.
	* If "[Auditing AI](https://mitpress.mit.edu/9780262051729/auditing-ai/)" is available from MIT Press, we may use chapters from that book instead
* Bullwinkel, B., Minnich, A., Chawla, S., Lopez, G., Pouliot, M., Maxwell, W., ... & Russinovich, M. (2025). [Lessons from red teaming 100 generative ai products](https://arxiv.org/abs/2501.07238). arXiv preprint arXiv:2501.07238.

Graduate student reading:

* Gerchick, M., Jegede, T., Shah, T., Gutierrez, A., Beiers, S., Shemtov, N., ... & Horowitz, A. (2023, June). [The devil is in the details: Interrogating values embedded in the allegheny family screening tool](https://dl.acm.org/doi/abs/10.1145/3593013.3594081). In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (pp. 1292-1310).

Before class, write how you would test the AI system you want to study for an outcome that matters to you.

---

#### 7.1 Methods: Investigating and Describing an Incident
So far this semester, we have looked at simple incident reports such as the systems used by the U.S. National Transportation Safety Board to report a crash— introduced in the 1960s. Since then, scholars have developed more comprehensive ideas about how to map and describe failures. How far should we go toward offering more complex models? And how much should we understand/engage with the politics at play?

In this class, we're going to learn more about those systems and work through the case of the Richmond CA Refinery fire of 2012. We're also going to consider how incident reporting fits into the wider politics of error reporting and the challenges of academic access to sensitive information.

* [Animation of Fire at Chefron's Richmond Refinery, August 6, 2012](https://www.youtube.com/watch?v=QiILbGbk8Qk). US Chemical Safety and Hazard Investigation Board.
* Sadisivam, N. (2021) [A California law gave the people power to cut pollution. Why isn’t it working?](https://grist.org/equity/ab617-richmond-california-chevron-refinery-air-monitoring/). Grist.
* Yousefi, A., Rodriguez Hernandez, M., & Lopez Peña, V. (2019). [Systemic accident analysis models: A comparison study between AcciMap, FRAM, and STAMP](https://aiche.onlinelibrary.wiley.com/doi/10.1002/prs.12002). Process Safety Progress, 38(2), e12002. (in Canvas)

Graduate student reading:

* Larsen, R. (2022). [‘Information Pressures’ and the Facebook Files: Navigating questions around leaked platform data](https://www.tandfonline.com/doi/full/10.1080/21670811.2022.2087099). Digital Journalism, 10(9), 1591-1603.

Before the class, reflect on what might have been different if someone had done a systematic analysis of the type of alleged AI failure you are hoping to study.

---

#### 7.2 Finalizing Student Projects
In your final-project teams, students will present a 3 minute presentation to summarize and analyze an issue in human-algorithm feedback that you would like to work on for your final. Fellow students will submit feedback and suggestions.

---

#### 8.1 Methods: Causality

* Odgers, C. L. (2024). [The great rewiring: is social media really behind an epidemic of teenage mental illness?](https://ideas.repec.org/a/nat/nature/v628y2024i8006d10.1038_d41586-024-00902-2.html) Nature, 628(8006), 29-30.
* Matias, N., & Penney, J. (2025). [Science and Causality in Technology Litigation](https://tsjournal.org/index.php/jots/article/view/233). Journal of Online Trust and Safety, 2(5).

Graduate student reading:

* Office of the Surgeon General. (2023). [Social media and youth mental health: The US Surgeon general’s advisory](https://pubmed.ncbi.nlm.nih.gov/37721985/). U.S. Department of Health and Human Services.

In this class, we will discuss the question of causality and methods for establishing the different kinds of causality that might be important to answer practical questions about technology benefits, harms, and interventions to transform them. For your project topic:

* Identify the kinds of causality that matter
* Identify hurdles to obtaining those causal inferences
* Guess how many scientists, studies, and years you think it might take to arrive at confident causal claims about the harm you are studying

---

#### 8.2 How Can Evidence Inform Governance?
What is evidence-based governance and how might research contribute (or not) to policy? In this session, we'll discuss a model where researchers are advisors, versus a model where researchers are producing evidence that is used in courts and other contestational settings.

* Spruijt, P., Knol, A. B., Vasileiadou, E., Devilee, J., Lebret, E., & Petersen, A. C. (2014). [Roles of scientists as policy advisers on complex issues: A literature review](https://www.sciencedirect.com/science/article/pii/S1462901114000598). Environmental Science & Policy, 40, 16-25.
* Systemic Justice (2023) [Strategic Litigation: A Guide for Legal Action](https://systemicjustice.ngo/community-toolkit/). Systemic Justice.

Graduate Student Reading:

* Weiss, C. H. (1979). [The many meanings of research utilization](https://www.jstor.org/stable/3109916). Public administration review, 39(5), 426-431.

Questions for discussion in Canvas and in class:
* For your team's final project area:

   * What kind of evidence might be needed for policymaking
   * Is that kind of evidence possible yet?
   * Does the evidence already exist?
   * How does evidence already influence policy, if at all?
   * What forces are preventing evidence from being available?-->

<!--Feedback Pattern: Chilling Effects on Rights
People are often influenced by algorithms by design, but algorithms can also have side effects. Algorithms can also cause a chilling effect when people are influenced against taking legally-protected actions such as avoiding certain streets or self-censoring their political views.

* Schauer, F. (1978). [Fear, Risk and the First Amendment: Unraveling the Chilling Effect](https://scholarship.law.wm.edu/facpubs/879/). Boston University Law Review, 58(5), 685–732.
* Brayne, S. (2014). Surveillance and system avoidance: Criminal justice contact and institutional attachment. American Sociological Review, 79(3), 367–391.
* (graduate student) Matias, J. N., Mou, M. E., Penney, J., & Klein, M. (2020). [Do Automated Legal Threats Reduce Freedom of Expression Online? Preliminary Results from a Natural Experiment](https://osf.io/nc7e2/). https://osf.io/nc7e2/
-->

<!--#### Thurs 03-24 Journalism and Policy: Hype Cycles or Accountability?
* (TBD: pick a recent example of tech accountability journalism)
* Orben, A. (2020). [The Sisyphean cycle of technology panics](https://journals.sagepub.com/doi/full/10.1177/1745691620919372). Perspectives on Psychological Science, 15(5), 1143-1157.
* (graduate students) Bossetta, M. (2020) [Scandalous Design: How Social Media Platforms’ Responses to Scandal Impacts Campaigns and Elections](https://journals.sagepub.com/doi/10.1177/2056305120924777). Social Media + Society, 6(2).-->
<!--
---
#### 8.2 Policy Topic: What Is Policy Anyway?
We've been talking about policy and governance in the abstract, but what is it really, and how is that changing in a world where algorithms are also being given governance power?

* Cairney, P. (2019). "[What is Policy and policymaking](https://paulcairney.files.wordpress.com/2019/03/chapter-2-upp-2nd-ed-8.3.19.pdf)" from Understanding public policy: theories and issues. Red Globe Press.
* Gillespie, T. (2017). [Governance of and by platforms](https://culturedigitally.org/2016/06/governance-of-and-by-platforms/). SAGE handbook of social media, 254-278.

Graduate Student reading: 

* Lessig, L. (2009). "Code is Law" from [Code: And other laws of cyberspace](https://lessig.org/product/code). Basic Books.

Questions for discussion:

* What is a tech policy, really?
* What is a platform policy, really?
* What kind of policy outcomes can be achieved (or not) with code?

---
-->

---


#### 9.1 Team Check-In

In this class, we will:

* Support group collaboration on student projects
* Discuss common questions about projects and final proposals

---


#### 9.2 Guest Panel: Processing Posthumous Digital Remains

In this class, we will hear from a group of scientists developing a center that can receive posthumous digital remains. 

* Orben, A., & Matias, J. N. (2025). [Fixing the science of digital technology harms](https://www.science.org/doi/full/10.1126/science.adt6807). Science, 388(6743), 152-155.


---

#### SPRING BREAK (Syllabus Adjustment Check-In)

---

<!--
#### Tues 04-05 Research Methods: Lab Tests and Simulations
Is it possible to anticipate problems with human-algorithm behavior before they happen? How can we design research methods to do this anticipatory work?

* Lucherini, E., Sun, M., Winecoff, A., & Narayanan, A. (2021). [T-RECS: A Simulation Tool to Study the Societal Impact of Recommender Systems](http://arxiv.org/abs/2107.08959). ArXiv:2107.08959 [Cs]. http://arxiv.org/abs/2107.08959
* (graduate students) Ie, E., Hsu, C., Mladenov, M., Jain, V., Narvekar, S., Wang, J., Wu, R., & Boutilier, C. (2019). [RecSim: A Configurable Simulation Platform for Recommender Systems]( http://arxiv.org/abs/1909.04847). ArXiv:1909.04847 [Cs, Stat]. 

#### Thurs 04-07 Research Methods: Algorithm Impact Assessments
How can we integrate evidence-creation into the policy process? One option is to borrow the idea of impact assessments from the area of environmental risk management.

* Moss, E., Watkins, E. A., Singh, R., Elish, M. C., & Metcalf, J. (2021). [Assembling Accountability: Algorithmic Impact Assessment for the Public Interest](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3877437). Available at SSRN 3877437.
* (TBD) example of an algorithmic impact assessment, if such a thing exists yet!
-->

#### 10.1 Policy Topic: How Does Evidence Actually Inform Policy?
In previous classes, we discussed theories for how evidence could inform policy. How does it really happen?

* **Each Team** share on Canvas an example of a piece of research, journalism, or activism that genuinely impacted policy (ideally related to your project), and comment on how it did so. A especially high quality submission will be able to cite an example where someone made a claim about the relationship between the corporate/government policy change and the project. This will require some research beyond the single article or project page. Comment on other team's examples, come prepared to discuss your team's example, and be prepared to ask questions and discuss other team's examples.

<!--
* Examples and readings to consider if you're looking for examples:

  * Edelman, B. G., & Luca, M. (2014). [Digital discrimination: The case of Airbnb.com](https://www.hbs.edu/ris/Publication%20Files/Airbnb_92dd6086-6e46-4eaf-9cea-60fe5ba3c596.pdf). Harvard Business School NOM Unit Working Paper, (14-054).
  * Buolamwini, J., & Gebru, T. (2018, January). [Gender shades: Intersectional accuracy disparities in commercial gender classification](https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf). In Conference on fairness, accountability and transparency (pp. 77-91). PMLR.
  * Penney, J. W. (2016). [Chilling effects: Online surveillance and Wikipedia use](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2769645). Berkeley Tech. LJ, 31, 117.
  * Matias, J. N. (2019). Preventing harassment and increasing group participation through social norms in 2,190 online science discussions. Proceedings of the National Academy of Sciences, 116(20), 9785-9789.-->

Graduate Student Reading (choose one):  

  * Specter, M., Sellars, A. (2025 - Forthcoming) [Software Screws Around, Reverse Engineering Finds Out: How Independent, Adversarial Research Informs Government Regulation](https://ftcreverse.engineering/). 
  * Contandriopoulos, D., Lemire, M., Denis, J. L., & Tremblay, É. (2010). [Knowledge exchange processes in organizations and policy arenas: a narrative systematic review of the literature](https://pubmed.ncbi.nlm.nih.gov/21166865/). The Milbank Quarterly, 88(4), 444-483.

---

#### 10.2 Policy Topic: Algorithmic Literacy
In order for democratic societies to govern a problem, people need to know about it and care enough to do something. How does the public understand and care about human-algorithm feedback?

* Druga, S., Christoph, F., & Ko, A. J. (2022). [Family as a Third Space for AI Literacies: How do children and parents learn about AI together?](https://faculty.washington.edu/ajko/papers/Druga2022FamilyAILiteracy.pdf).
* Rainie, L., Anderson, J. (2017) "The need grows for algorithmic literacy, transparency and oversight" from [Code-Dependent: Pros and Cons of the Algorithm Age](https://www.pewresearch.org/internet/2017/02/08/code-dependent-pros-and-cons-of-the-algorithm-age/). Pew Research Center
<!--* (possible invited guests: Baobao Zhang, Natalize Bazarova)-->

Graduate student reading:

* Menon, A., & Zhang, B. (2025). [Future Shock or Future Shrug? Public Responses to Varied Artificial Intelligence Development Timelines](https://www.journals.uchicago.edu/doi/pdf/10.1086/739200). The Journal of Politics.


Discussion questions:

* If the design of algorithms is controlled by engineers and most people aren't engineers, how would algorithm literacy actually make a difference in people's lives if at all?
* What kinds of knowledge about algorithms does a democratic public need?
* How could your project area be transformed by algorithmic literacy — and how not?

<!--* (possible invited guests: Mozilla, AccessNow, Center for Democracy and Technology, Electronic Frontier Foundation, Knight First Amendment Center, ACLU)


<!--
#### Tues 04-19 Policy Topic: Can Tech Ethics Prevent Harms?
Advocates of tech ethics argue that if we teach ethics to engineers, society will be more protected from the harms associated with algorithms. How much can that help?

* Silbey, S. S. (2009). [Taming Prometheus: Talk about safety and culture](https://anthropology.mit.edu/sites/default/files/documents/silbey_safety_culture.pdf). Annual Review of Sociology, 35, 341–369.
* Zhang, B., Anderljung, M., Kahn, L., Dreksler, N., Horowitz, M. C., & Dafoe, A. (2021). [Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers](https://arxiv.org/pdf/2105.02117.pdf). arXiv preprint arXiv:2105.02117.

Questions for discussion on Canvas and in class:
* For the algorithm harms you and your team are thinking about:
   * What does it mean for things to be described as an "accident" and how does that imply certain kinds of solutions?
   * How do people place responsibility on individual engineers?
   * When do you think institutions rather than individuals should be responsible?
* What questions do you have about terms? Silbey is trying to summarize ~150 years of debates and regulation, so there's a lot going on!
   * What is an example of a "moral entrepreneur?"
   * What is "safety culture" and how do people identify culture as responsible for disasters and accidents?

-->
<!--#### Tues 04-19 Policy Topic: Conversation with Advocacy Groups
In this session, we will have a conversation with representatives of thinktanks and advocacy groups working on algorithm-related policy topics in Washington DC.

<y!--* (possible invited guests: Mozilla, AccessNow, Center for Democracy and Technology, Electronic Frontier Foundation, Knight First Amendment Center, ACLU) -->

---

#### 11.1 Policy Topic: Public Engagement with Incident Reporting

If incident reporting is to work, the public need to understand incidents. In this class, we will look at the history of the Dark Patterns Tip Line.

* Nguyen, Stephanie (2021) [Key learnings from the Dark Patterns Tip Line](https://ritaallen.org/stories/key-learnings-from-the-dark-patterns-tip-line/). Rita Allen Foundation
* King, J., & Stephan, A. (2021). [Regulating Privacy Dark Patterns in Practice-Drawing Inspiration from the California Privacy Rights Act](https://georgetownlawtechreview.org/regulating-privacy-dark-patterns-in-practice-drawing-inspiration-from-california-privacy-rights-act/GLTR-09-2021/). Georgetown Law Technology Review, 5(2), 250-276.

Graduate Student Reading:

* Blake, C., Rhanor, A., & Pajic, C. (2020). [The demographics of citizen science participation and its implications for data quality and environmental justice](https://theoryandpractice.citizenscienceassociation.org/articles/10.5334/cstp.320?_rsc=8l4u7). Citizen Science: Theory and Practice, 5(1).

Questions for discussion on Canvas and in class:

* For your team's topic, how might it be possible to ask the public to report incidents?
* Would the public be able to notice and distinguish possible incidents?
* What kind of public awareness will be necessary?
* How might inequalities in incident reporting influence who gets served by policy?

<!--#### Thurs 04-16 Policy Topic: Protecting Algorithms from Bad People with Content Moderation

If algorithms are just math formulas that offer a reflection of a broken world, one way to govern algorithms is to protect them from bad people. What does that look like?

* Clegg, N. (2021, March 31). [You and the Algorithm: It Takes Two to Tango](https://nickclegg.medium.com/you-and-the-algorithm-it-takes-two-to-tango-7722b19aa1c2). Medium. 
* Roberts, S. T. (2019). "Understanding Commercial Content Moderation," from Behind the screen. Yale University Press.
-->

---

#### 11.2 Policy Topic: Creating Change With Courts
What influence can courts have on the behavior of companies, populations, and algorithms? 

* Charles, S. (2020, January 27). [CPD decommissions ‘Strategic Subject List.’](https://chicago.suntimes.com/city-hall/2020/1/27/21084030/chicago-police-strategic-subject-list-party-to-violence-inspector-general-joe-ferguson) Chicago Sun-Times. 
* Kaplan, J. (2017). [Predictive Policing and the Long Road to Transparency](https://southsideweekly.com/predictive-policing-long-road-transparency/)

Graduat Student Reading:

* Jillson, Elisa. (2021) [Aiming for truth, fairness, and equity in your company's use of AI](https://www.ftc.gov/news-events/blogs/business-blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai). Federal Trade Commission.

Questions to consider:

* What role did courts play in the policy debates about the Strategic Subjects List?
* How can laws and agencies influence what courts are able to do?
* How might courts be important to the project you're working on?

---

#### 12.1 Final Project Check-in
In the class, teams will be supported to meet in person, workshop your ideas with peers, and get feedback on your project progress.

---

#### 12.2 Case Study: TBD

In this class, we will review a consequential case study that made it into the courts and hear from guests who were involved on different sides of the case. 

Readings: TBD


<!--Guests:

* Susan Benesch
* Erin Kissane
-->
<!--Readings:

* Kissane, E. (2023) [Meta in Myanmar, Part 1: The Setup](https://erinkissane.com/meta-in-myanmar-part-i-the-setup). Erinkissane.com
* Kissane, E. (2023) [Meta in Myanmar, Part 2: The Crisis](https://erinkissane.com/meta-in-myanmar-part-ii-the-crisis). Erinkissane.com
* Kissane, E. (2023) [Meta in Myanmar, Part 3: The Inside View](https://erinkissane.com/meta-in-myanmar-part-iii-the-inside-view). Erinkissane.com-->



<!--#### Thurs 04-28 Research Methods: Citizen Science and Field Experiments
How can we learn the safety of algorithms or the effectiveness of policies in real-world situations—while still limiting the risks? Field experiments, when conducted with public consent and careful oversight, enable researchers to audit systems and test policies in controlled circumstances with as few people as possible.

* Matias, J. N. (2015, June 8). [How Do You Fix a Broken System That Isn’t Yours to Repair?](https://www.theatlantic.com/technology/archive/2015/06/the-tragedy-of-the-digital-commons/395129/) The Atlantic.
* Campbell, D. T. (1969). [Reforms as experiments](https://www.sfu.ca/~palys/Campbell-1969-ReformsAsExperiments.pdf). American psychologist, 24(4), 409.
* (gradstudents) TBD-->

---

#### 13.1 Final Presentations Part I
In this session, teams will give a final presentation of their project, with an opportunity to receive feedback from peers.

In the discussion, please post in advance a question for feedback you would like your peers to consider during your session in the class.

---

#### 13.2 Final Presentations Part II
In this session, teams will give a final presentation of their project, with an opportunity to receive feedback from peers.

In the discussion, please post in advance a question for feedback you would like your peers to consider during your session in the class.

---

#### 14.1 Reflections on Learning & the Future of Algorithms and Society
In this session, we will reflect on the topics in the class. Please submit a discussion post, which will inform our group conversation:

* Name something you believed at the beginning of the course
* Reflect on how your thinking about that question has changed
* Name one way this topic might continue to be relevant to your future life, whether in your role as citizen or your work

---


#### Final Project Deadline: TBD 

---


## Course Practices and Policies

### Weekly Workload

Before each class, I will assign **two readings** I expect you to read before class. As part of class participation, you will submit a reaction comment on one of the readings to the relevant discussion on Canvas and respond to at least one other student's comment by 9pm Eastern the night before class. Please come to class with:

* The ability to summarize:
  * the goal/question of the paper
  * the field or ecosystem that the author is in
  * what constitutes an advancement in that field
  * how the paper advances the conversation
  * a question or observation that links the paper to the theme of the day or the theme of the course 
  * you can choose to respond to one of the prompt questions, but are not obligated to

Each week, students are expected to post at least one news article to Canvas add at least one comment in response to one other student's posted link.

Graduate students will have additional readings. Graduate students will also rotate responsibility for summarizing their reading to the class. Summarizing the reading will involve:

* Creating 2-3 slides for the reading
  * Slide one: introducing the core question of the paper, and the authors
  * Slide two: summarize the methods and findings of the paper
  * Slide three: one to two discussion questions that link the paper to the theme of the class
* Alt: if the author of the paper is a guest speaker, the graduate student will interview the guest

Since this is a discussion course, attendance is expected.

### Participation in Discussions
Please post discussions about readings to Canvas. 

Whenever the course has assigned reading for a session, you are expected to post at least one top-level comment and one reply to someone else's comment by the end of the day before class. Participation will be 30% of your course grade.

### Team Progress Reports
During the project period of the class, teams will submit on Canvas a weekly progress report no more than one page long, as group homework. This progress report will be graded 0/1 based on whether it was submitted. Reports should include the following details (a sample template is available here):

* What your team made progress on
* What your team is doing next
* Who contributed what
* Where your team is stuck
* Any updates to your timeline

### Formats
Written assignments should be uploaded to Canvas in one of the following formats:

* PDF
* Text file
* Word-compatible document

Slide decks should be submitted in one of the following formats:

* PowerPoint
* Keynote
* Google Slides
* A Markdown slide presentation system (such as Marp)
* PDF

In cases where students choose to submit code or analysis as part of a project, I can accept assignments in the following languages:

* R (including R Markdown or Sweave)
	* For more guidance on R, see [this excellent Cornell course](https://nt246.github.io/NTRES-6100-data-science/syllabus.html), which has full public materials.
* Python
* Ruby

### Group Work and Academic Integrity
I expect students to follow the [Cornell University Code of Academic Integrity](https://www.library.cornell.edu/research/citation/code). You should submit your work as your own, cite sources and outside assistance, and credit people for their contributions.

<img src="https://teaching.cornell.edu/sites/default/files/2025-08/AT.png" style="float:right; width:100px;"/>

This class includes group work and individual assignments. On group projects, you are encouraged to work together on the activities for the class, but you are only only able to put your name on projects to which you made an intellectual contribution. If you have any questions about what is appropriate, ask me.

Because my main goal is for you to develop the ability to read, think, develop ideas, and analyze data, this course is an "[Approved Tools Only](https://teaching.cornell.edu/generative-artificial-intelligence/ai-course-policy-icons)" class for Generative AI. If you are thinking about using a generative tool, please consult the professor in advance, consult any team-mates, and acknowledge its use in any submissions. At the beginning of the class, I will issue a survey for students to share the tools you find helpful and will pre-approve ones that are compatible with the learning goals of the course.

As a [sign of respect for others](https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity), please verify your own work before submitting it as an assignment or sharing it with a team-mate.

#### Approved Generative AI Tools:
The list of approved AI tools will be updated in this table. 

| Purpose      | Tool category       | Use Suggestion |
|-------|-------|-------|
|Writing and editing | Grammar checking tools | Use to review text after writing |
|Information retrieval| Search engines <br/>Information discovery apps | Use for discovery and inspiration, but always verify by reading anything you cite|
|-------|-------|



<!-- Students are responsible for the reliability and accuracy of all work. And -->

### Recordings and Classroom Privacy
Since this is a small course focused on highly sensitive issues, we need to take special care to encourage and protect everyone's capacity to speak freely. Each person in this class is expected to respect the principles of academic freedom for instructors and classmates and will maintain the privacy of the classroom environment, as outlined in Cornell's Code of Academic Integrity.

This commitment to building respect and trust in the classroom means members of this class will not: record, photograph, or share online any interactions that involve classmates or any member of the teaching team. Students will also respect the intellectual property rights of the instructor, and will not share or otherwise make accessible any course materials to anyone not enrolled in the course, without the instructor’s written permission.

This policy is not meant to restrict students’ ability to use classroom recordings in ways beneficial to their learning. Students who may benefit from recorded lectures and lecture playback, including students who use English as an additional language or who have accommodations from SDS, should speak to the course instructor to maintain transparency and trust in the classroom. Students approved to record lectures are expected to maintain the respect and privacy of the learning environment, as stated above.


### Grading
I use the following grading scale, derived from Matt Salganik's grading practices.

All grades are final. There will not be any make-up or extra credit assignments offered.

Per university procedures, I will only give a grade of A+ in exceptional circumstances.

Letter Grade	Numeric Grade

* A	93 - 100
* A-	90 - 92.99
* B+	87 - 89.99
* B	83 - 86.99
* B-	80 - 82.99
* C+	77 - 79.99
* C	73 - 76.99
* C-	70 - 72.99
* D	60 - 66.99
* F	0 - 59.99

### Grace Period
Late Submissions: All assignments have an automatic one-day grace period. On time and early papers are always encouraged and will be graded the same week. Students can turn in the assignment up to a day late, no questions asked, with the expectation that it could take substantially longer to receive a grade and feedback. After that, an automatic one grade (A to B, B to C, etc) is dropped on the assignment.

If you turned in an assignment on time and haven't received a grade within the expected period, please contact the instructor in case of a technical glitch.

<!--### COVID Policies
Students are expected to follow [Cornell COVID policies for students](https://covid.cornell.edu/students/), including vaccination and masking. 
-->
### Accommodations
I am committed to working with students with recognized SDSes to ensure you have the best possible class experience. Please [follow these steps to get started](https://sds.cornell.edu/get-started).

### Health Accommodation for Professor Matias
As part of a [medical condition of my own](https://natematias.com/portfolio/2021-06-06-ithaca-allergies/), I may on rare occasion need accommodations myself. I will introduce the details of this accommodation and related processes in the first class.

### Counseling Resources
This class is covering some very difficult topicsthat relate to severe harms. If you are someone who wrestles with mental health issues, I encourage you to take a moment to ensure that you have the support you need to take this class effectively. If you're unsure what kind of support would be helpful, we can discuss ideas in office hours.

It is common for students to experience stressful events at some point during graduate school. Students sometimes experience depression, anxiety, family stress, the loss of loved ones, financial strain, and other stressors. It is perfectly normal for students to seek the service of mental health professionals to provide them with support and skills to cope with these experiences. Below I have provided the contact information for some of the mental health services available to Cornell University students so that you will know where you can go if you or a friend would like to take advantage of these resources.

Cornell Health: 110 Ho Plaza, Ithaca NY. Phone: 607-255-5155.

### Acknowledgments

I am grateful to Lucas Wright for his contributions to a review article that this course is based on, and to the Human-Algorithm-Behavior Research Collective for input. Several sections of these policies have been inspired by syllabi from Matthew Salganik, Adrienne Keene, and Neil Lewis Jr.

### Attention to Detail
Reading instructions carefully, paying attention to detail, and planning your time wisely are all essential for thriving in this course. All of these will be rewarded with learning, and in the case of this syllabus—this link to a [cute baby owl getting a massage](https://www.youtube.com/shorts/CScKkXVW1Sc).
